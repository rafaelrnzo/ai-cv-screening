# ğŸ§  AI Screening Service (RAG-Powered Candidate Evaluator)
## Overview
The **AI Screening Service** is a backend application built with **FastAPI**, designed to automatically evaluate candidate CVs and project reports based on predefined job descriptions and rubrics.  
It combines **Google Gemini 2.5 Flash**, **BAAI/bge-m3 embeddings**, and **Redis Vector Search (RediSearch)** to deliver a robust, contextual **RAG (Retrieval-Augmented Generation)** pipeline that performs accurate candidate evaluation at scale.

This project supports:
- Asynchronous job orchestration
- Document embedding and semantic search
- Modular, production-ready FastAPI structure
- Configurable environment variables
- Portable deployment via Docker Compose

---

## ğŸ—ï¸ Architecture
```markdown
app/
 â”œâ”€â”€ api/
 â”‚   â”œâ”€â”€ schemas/            # Pydantic models for requests/responses
 â”‚   â”œâ”€â”€ v1/
 â”‚   â”‚   â”œâ”€â”€ endpoints/      # FastAPI route handlers
 â”‚   â”‚   â””â”€â”€ routers.py
 â”‚   â””â”€â”€ handlers/           # Optional LLM orchestration layer
 â”œâ”€â”€ core/
 â”‚   â”œâ”€â”€ config.py           # Pydantic settings + .env loader
 â”‚   â”œâ”€â”€ embedding_client.py # SentenceTransformer client
 â”‚   â”œâ”€â”€ llm_client.py       # Gemini API wrapper
 â”‚   â”œâ”€â”€ redis_client.py     # Redis & RediSearch connection utilities
 â”œâ”€â”€ services/
 â”‚   â”œâ”€â”€ ingest_service.py   # Seeding & indexing ground truth docs
 â”‚   â”œâ”€â”€ pipeline_service.py # Main AI evaluation orchestration
 â”‚   â””â”€â”€ search_service.py   # RAG context retrieval logic
 â”œâ”€â”€ utils/
 â”‚   â”œâ”€â”€ file_io.py          # File reading + PDF parsing (pypdf)
 â”‚   â””â”€â”€ logger.py           # Logging utilities
 â”œâ”€â”€ main.py                 # FastAPI entrypoint
 â”œâ”€â”€ tests/                  # (Planned) Unit & integration tests
 â””â”€â”€ infra/                  # Dockerfile, compose, and scripts
````

---

## âš™ï¸ Core Features

* **Google Gemini 2.5 Flash** â€“ Generates structured evaluation JSON with high linguistic precision
* **BAAI/bge-m3 embeddings** â€“ Efficient multilingual semantic retrieval
* **Redis Stack (RediSearch)** â€“ Vector storage for millisecond-level lookup
* **Async background jobs** â€“ Handles long-running evaluation requests cleanly
* **PostgreSQL persistence** â€“ Tracks evaluations and logs for future audits
* **Modular architecture** â€“ Each layer is isolated, testable, and maintainable

---

## ğŸš€ Getting Started

You can run the service in **two modes**:
* **A. Local Python (for dev & debugging)**
* **B. Docker Compose (recommended for deployment)**

---

### ğŸ§© A. Local Setup (Python / venv)

#### 1. Clone the repository

```bash
git clone https://github.com/rafaelrnzo/ai-screening-service.git
cd ai-screening-service
```

#### 2. Create virtual environment

```bash
python -m venv envAI
source envAI/bin/activate        # macOS/Linux
envAI\Scripts\activate           # Windows
```

#### 3. Install dependencies

```bash
pip install -r requirements.txt
```

#### 4. Configure environment variables

Create a `.env` file at project root:

```env
APP_NAME=AI Screening Service
SERVER_HOST=0.0.0.0
SERVER_PORT=8004

GOOGLE_API_KEY=YOUR_KEY
GEMINI_MODEL=gemini-2.5-flash
EMBEDDING_MODEL=BAAI/bge-m3

REDIS_URL=redis://localhost:6379/0
INDEX_NAME=gt_idx
DOC_PREFIX=gt:

POSTGRES_USER=admin
POSTGRES_PASSWORD=admin.admin
POSTGRES_DB=ai_screening
POSTGRES_HOST=localhost
POSTGRES_PORT=5432
DATABASE_URL=postgresql+psycopg2://admin:admin.admin@localhost:5432/ai_screening

UPLOAD_DIR=./data/uploads
GROUND_DIR=./data/ground_truth

BACKEND_CORS_ORIGINS=["http://localhost:3000","http://127.0.0.1:5173"]
```

#### 5. Start Redis Stack

```bash
docker run -d \
  --name redis-stack \
  -p 6379:6379 \
  -p 8001:8001 \
  redis/redis-stack:7.4.0-v1
```

#### 6. Run the app

```bash
uvicorn app.main:app --reload --host 0.0.0.0 --port 8004
```

Visit [http://localhost:8004/docs](http://localhost:8004/docs)

---

### ğŸ³ B. Setup via Docker Compose (Recommended)

This mode runs **FastAPI**, **Postgres**, and **Redis Stack** together with a single command.

#### 1. Verify infra files

```
infra/docker/Dockerfile
docker-compose.yml
.env
```

#### 2. Build & start services

```bash
docker compose build
docker compose up -d
docker compose logs -f ai-screening-api
```

Access API â†’ [http://localhost:8004/docs](http://localhost:8004/docs)

#### 3. Stop & clean

```bash
docker compose down -v
```

---

## ğŸ“¡ API Endpoints

| Method | Endpoint                           | Description                |
| :----: | :--------------------------------- | :------------------------- |
|  `GET` | `/api/v1/health/`                  | Health check               |
| `POST` | `/api/v1/evaluate/upload`          | Upload CV & project report |
| `POST` | `/api/v1/evaluate`                 | Start evaluation job       |
|  `GET` | `/api/v1/evaluate/result/{job_id}` | Get job result/status      |

---

## ğŸ“˜ Example Workflow

```bash
# 1ï¸âƒ£ Upload candidate files
POST /api/v1/evaluate/upload
â†’ returns cv_id, report_id

# 2ï¸âƒ£ Start evaluation
POST /api/v1/evaluate
{
  "job_title": "Backend Engineer",
  "cv_id": "<uuid>",
  "report_id": "<uuid>"
}
â†’ returns job_id

# 3ï¸âƒ£ Check results
GET /api/v1/evaluate/result/<job_id>
â†’ returns match_rate, project_score, overall_summary
```

---

## ğŸ§  Evaluation Pipeline Flow

```mermaid
flowchart TD
  A[Upload CV & Report] --> B[Extract & Embed Documents]
  B --> C[Retrieve Context via RAG (Redis Vector Search)]
  C --> D[LLM Evaluation (Gemini 2.5 Flash)]
  D --> E[Aggregate Scores & Feedback]
  E --> F[Store Result in Postgres]
  F --> G[Return Job ID + Async Result Endpoint]
```

---

## ğŸ§© Technologies

| Layer       | Tech                     |
| ----------- | ------------------------ |
| Framework   | FastAPI                  |
| Vector DB   | Redis Stack (RediSearch) |
| Embeddings  | BAAI/bge-m3              |
| LLM         | Google Gemini 2.5 Flash  |
| Persistence | PostgreSQL               |
| Config      | Pydantic Settings        |
| Environment | Python 3.11+             |
| Deployment  | Docker & Docker Compose  |

---

## ğŸ§ª Future Improvements

* [ ] **OCR-based CV parsing** â€” Integrate `pytesseract` + `pdf2image` for scanned resumes
* [ ] **LoRA fine-tuning** â€” Improve response precision on custom job rubrics
* [ ] **Celery / Redis Queue** â€” True distributed job orchestration
* [ ] **Retry & fallback pipeline** â€” Backoff logic for Gemini timeouts or rate limits
* [ ] **Analytics dashboard** â€” View candidate scores, rubric trends, and reports
* [ ] **Caching layer** â€” Reuse embeddings for identical CVs or projects
* [ ] **Unit tests & CI/CD integration**
* [ ] **PDF parsing fallback** â€” Combine `pypdf` + OCR for maximum text recovery
* [ ] **Sentry / ELK integration** â€” Centralized error monitoring

---

## ğŸ‘¨â€ğŸ’» Author

**Rafael Lorenzo**
Backend Engineer | AI Engineer | Fullstack Developer

ğŸŒ [rafaelrnzo.vercel.app](https://rafaelrnzo.vercel.app)
ğŸ’¼ [github.com/rafaelrnzo](https://github.com/rafaelrnzo)
